# Scripts Directory

Automation scripts for the modded-nanogpt project.

## ğŸš€ Entropy Filtering Validation Scripts

### `run_complete_validation.sh` â­ **RECOMMENDED**

**Complete end-to-end validation pipeline** - Fully automated!

Runs everything from start to finish:
1. âœ… Train probe model (~30-45 min)
2. âœ… Score documents (~1-2 hours)
3. âœ… Filter dataset (~10-15 min)
4. âœ… Train baseline model (~10 min)
5. âœ… Train filtered model (~8 min)
6. âœ… Compare results automatically

**Usage:**
```bash
bash scripts/run_complete_validation.sh
```

**Total time:** ~3-4 hours (fully automated, no manual steps!)

**Output:**
- Probe model checkpoint
- Document scores JSON
- Filtered dataset
- Training logs for both experiments
- Automated comparison table

**Features:**
- âœ… Checks for existing files (can resume)
- âœ… Color-coded output
- âœ… Progress indicators
- âœ… Automatic result comparison
- âœ… Success/failure criteria check

---

### `run_entropy_validation.sh`

**Partial automation** - Stops before training experiments.

Runs only the data preparation steps:
1. âœ… Train probe model
2. âœ… Score documents
3. âœ… Filter dataset
4. âŒ Manual training required

**Usage:**
```bash
bash scripts/run_entropy_validation.sh
```

**When to use:**
- If you want to manually control training experiments
- If you want to inspect filtered data before training
- If you need to customize training parameters

---

## ğŸ“Š Comparison

| Feature | run_complete_validation.sh | run_entropy_validation.sh |
|---------|---------------------------|---------------------------|
| Probe training | âœ… Auto | âœ… Auto |
| Document scoring | âœ… Auto | âœ… Auto |
| Dataset filtering | âœ… Auto | âœ… Auto |
| Baseline training | âœ… Auto | âŒ Manual |
| Filtered training | âœ… Auto | âŒ Manual |
| Result comparison | âœ… Auto | âŒ Manual |
| **Total automation** | **100%** | **60%** |

---

## ğŸ¯ Quick Start

**For hands-off validation:**
```bash
# Just run this and come back in 3-4 hours!
bash scripts/run_complete_validation.sh
```

**For step-by-step control:**
```bash
# Run data prep only
bash scripts/run_entropy_validation.sh

# Then manually run training experiments
# (see local_reference/ENTROPY_FILTERING_GUIDE.md)
```

---

## ğŸ“ Output Files

Both scripts create:

```
checkpoints/
  â””â”€â”€ probe_model_100M.pt              # Trained probe model

data/
  â”œâ”€â”€ fineweb_scores_100M.json         # Document scores
  â””â”€â”€ fineweb10B_filtered_80pct/       # Filtered dataset
      â”œâ”€â”€ fineweb_train_000000.bin
      â”œâ”€â”€ fineweb_train_000001.bin
      â””â”€â”€ ...

logs/validation/                        # Only in complete script
  â”œâ”€â”€ baseline_raw.log                  # Baseline training log
  â””â”€â”€ filtered_80pct.log                # Filtered training log
```

---

## âš™ï¸ Configuration

Edit the scripts to customize:

**Data size:**
```bash
NUM_TOKENS=100000000        # 100M for quick validation
NUM_TOKENS=900000000        # 900M for full dataset
```

**Filtering aggressiveness:**
```bash
KEEP_FRACTION=0.80          # Conservative (20% pruning)
KEEP_FRACTION=0.70          # Moderate (30% pruning)
KEEP_FRACTION=0.60          # Aggressive (40% pruning)
```

**Training steps:**
```bash
BASELINE_STEPS=1000         # Quick validation
BASELINE_STEPS=2315         # Full training
```

**Scoring method:**
```bash
ALPHA=0.5                   # Balanced (NLL + word freq)
ALPHA=1.0                   # Model-only (NLL)
ALPHA=0.0                   # Frequency-only (no model)
```

---

## ğŸ”§ Advanced Usage

### Resume from checkpoint

Both scripts check for existing files:
- If probe checkpoint exists: Option to reuse
- If scores exist: Option to reuse
- If filtered data exists: Option to reuse

Just re-run the script!

### Parallel scoring

For faster scoring on multiple GPUs:
```bash
# Edit score_fineweb_entropy.py to process shards in parallel
# Or run multiple scoring jobs on different data shards
```

### Custom parameters

```bash
# Quick test on tiny dataset
NUM_TOKENS=10000000 bash scripts/run_complete_validation.sh

# More aggressive pruning
KEEP_FRACTION=0.60 bash scripts/run_complete_validation.sh
```

---

## ğŸ“š Documentation

For detailed information:
- **Usage guide:** `local_reference/ENTROPY_FILTERING_GUIDE.md`
- **Implementation:** `local_reference/IMPLEMENTATION_SUMMARY.md`
- **Research:** `local_reference/idea.md`

---

## âœ… Expected Results

After running `run_complete_validation.sh`, you should see:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Metric              â”‚ Baseline     â”‚ Filtered     â”‚ Change       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Training Data       â”‚ 100M tokens  â”‚ 80M tokens   â”‚ -20%         â”‚
â”‚ Training Steps      â”‚ 1000         â”‚ 800          â”‚ -20%         â”‚
â”‚ Training Time       â”‚ ~600000ms    â”‚ ~480000ms    â”‚ -20%         â”‚
â”‚ Final Val Loss      â”‚ 3.65         â”‚ 3.63         â”‚ -0.02 âœ“      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

âœ“âœ“âœ“ VALIDATION SUCCESSFUL! âœ“âœ“âœ“

Entropy filtering achieves same performance with 20% less data and compute!
```

---

## ğŸ› Troubleshooting

**Script fails with "file not found":**
- Check that you have FineWeb data: `ls data/fineweb10B/`
- Download data: `python data/cached_fineweb10B.py 9`

**Out of memory:**
- Reduce probe model batch size (edit script)
- Use smaller validation dataset

**Training fails:**
- Check logs in `logs/validation/`
- Ensure train_gpt_single.py works standalone first

---

## ğŸ‰ Success!

If validation succeeds, scale to full dataset:
1. Edit script: `NUM_TOKENS=900000000`
2. Run: `bash scripts/run_complete_validation.sh`
3. Wait ~12 hours for full pipeline
4. Enjoy 20% faster training! ğŸš€
